<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Backpropagation for an MLP classifier on MNIST</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="index.css" />
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Backpropagation for an MLP classifier on MNIST</h1>
<p class="date">2023 Dec 16</p>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#preliminaries" id="toc-preliminaries">Preliminaries</a></li>
<li><a href="#mlp-classification" id="toc-mlp-classification">MLP classification</a></li>
<li><a href="#the-best-part-was-when-he-said-its-calculus-time-and-calculused-all-over-those-guys"
id="toc-the-best-part-was-when-he-said-its-calculus-time-and-calculused-all-over-those-guys">the
best part was when he said “IT’S CALCULUS TIME” and calculused all over those guys</a></li>
</ul>
</nav>
<h1 id="preliminaries">Preliminaries</h1>
<p>Below we use row-major convention to make the math match the implementation. All vectors are row
vectors, which matches the usual data matrix <span class="math inline">\(X\)</span> being a vertical
stack of training example row vectors.</p>
<p>For</p>
<p><span class="math display">\[\begin{bmatrix} s_1, \ldots, s_n \end{bmatrix} =
\text{softmax}([x_1, \ldots, x_n]) = \begin{bmatrix} \frac{e^{x_i}}{\sum_{j=1}^n e^{x_j}} : i = 1,
\ldots, n \end{bmatrix}\]</span></p>
<p>then the softmax partial derivatives are:</p>
<p><span class="math display">\[\frac{\partial s_j}{x_i} = s_j (\delta_{ij} - s_i)\]</span></p>
<p>where <span class="math inline">\(\delta_{ij}\)</span> is the Kronecker delta. Since the softmax
Jacobian is symmetric, it is unchanged under row-major notation convention.</p>
<p>We use notation</p>
<p><span class="math display">\[\delta[k, :] := \begin{bmatrix} \delta_{k1} &amp; \ldots &amp;
\delta_{kn} \end{bmatrix}\]</span></p>
<p>to denote a row vector implementing an indicator variable for the <span
class="math inline">\(k\)</span>-th component (all components are 0 save for the <span
class="math inline">\(k\)</span>-th, which is 1). The length <span class="math inline">\(n\)</span>
is usually not explicitly notated, and is instead inferred from context.</p>
<p>The element-wise product between tensors of the same shape is the Hadamard product, denoted <span
class="math inline">\(\odot\)</span>.</p>
<p>The cross-entropy between the true random variable <span class="math inline">\(X \sim p\)</span>
and an estimate <span class="math inline">\(\hat{X} \sim q\)</span>, where <span
class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> are finite probability
distributions <span class="math inline">\(p = [p_1, \ldots, p_n], q = [q_1, \ldots, q_n]\)</span>,
is:</p>
<p><span class="math display">\[CE(p, q) := H(X, \hat{X}) := \mathbb{E}_{X \sim p}[- \log(\hat{X})]
= - \sum_{i=1}^n p_i \log(q_i)\]</span></p>
<p>This is the average information content from estimating a distribution <span
class="math inline">\(q\)</span> when the true distribution is <span
class="math inline">\(p\)</span>. Alternatively: the average negative log probability of our
estimate of the outcomes, averaged with respect to the true probabilities.</p>
<p>We define, for <span class="math inline">\(n \in \mathbb{N}\)</span>:</p>
<p><span class="math display">\[[n] := \{1, \ldots, n \}\]</span></p>
<h1 id="mlp-classification">MLP classification</h1>
<p>Let <span class="math inline">\(d = 28^2 = 784\)</span> be the input size, <span
class="math inline">\(h\)</span> be the hidden size, <span class="math inline">\(c = 10\)</span> be
the output size (number of classes), and <span class="math inline">\(m\)</span> be the minibatch
size. Let <span class="math inline">\(X \in \mathbb{R}^{m \times d}\)</span> be a minibatch of
inputs. Then the first layer pre-activation and activation are:</p>
<p><span class="math display">\[A^{(1)} = X W^{(1)} + b^{(1)}\]</span></p>
<p><span class="math display">\[Z^{(1)} = \phi(A^{(1)})\]</span></p>
<p>where <span class="math inline">\(W^{(1)} \in \mathbb{R}^{d \times h}\)</span> is the weight
matrix of the fully-connected hidden layer, <span class="math inline">\(b^{(1)} \in
\mathbb{R}^h\)</span> is the bias vector, and <span class="math inline">\(\phi\)</span> is a
differentiable activation function <span class="math inline">\(\phi\)</span> applied component-wise.
The addition in the pre-activation broadcasts the bias vector to all rows of <span
class="math inline">\(X W^{(1)}\)</span>.</p>
<p>The output layer is another linear layer (<span class="math inline">\(W^{(2)} \in \mathbb{R}^{h
\times c}, b^{(2)} \in \mathbb{R}^c\)</span>), followed by a softmax:</p>
<p><span class="math display">\[A^{(2)} = Z^{(1)} W^{(2)} + b^{(2)}\]</span></p>
<p><span class="math display">\[Z^{(2)} = \text{softmax}(A^{(2)})\]</span></p>
<p>As a result, <span class="math inline">\(A^{(2)}, Z^{(2)} \in \mathbb{R}^{m \times c}\)</span>.
Each row in <span class="math inline">\(A^{(2)}_{[i,:]}\)</span> is the <em>logits</em> vector for
example <span class="math inline">\(i\)</span> in the minibatch, while <span
class="math inline">\(Z^{(2)}_{[i,:]}\)</span> is the prediction for example <span
class="math inline">\(i\)</span>.</p>
<p>For each minibatch, there is also a (<span class="math inline">\(m \times 1\)</span>) column
vector of labels <span class="math inline">\(y^\top\)</span>, where each label <span
class="math inline">\(y_i \in [c]\)</span>. One-hot encoding of each <span
class="math inline">\(y_i\)</span> into a <span class="math inline">\(1 \times c\)</span> vector
<span class="math inline">\([y_{i1}, \ldots, y_{ic}]\)</span> turns it into a probability
distribution over the <span class="math inline">\(c\)</span> classes, allowing us to apply
cross-entropy between each logits vector <span class="math inline">\(Z^{(2)}_i\)</span> (the
network’s prediction of the class for each example) and the ground-truth label <span
class="math inline">\(y_i\)</span>.</p>
<p>We further define <span class="math inline">\(Y := [y_{ij}]_{(i,j) \in [m] \times [c]}\)</span>,
i.e. the <span class="math inline">\(i\)</span>-th row of <span class="math inline">\(Y\)</span> is
<span class="math inline">\(Y_{[i,:]} = \delta[y_i, :]\)</span>. Therefore we obtain the
single-example loss:</p>
<p><span class="math display">\[\mathcal{L}(Z^{(2)}_{[i,:]}, y_i) := CE(Y_{[i,:]}, Z^{(2)}_{[i,:]})
= - \log(Z^{(2)}_{[i, y_i]})\]</span></p>
<p>Therefore, with respect to the minibatch <span class="math inline">\((X, y^\top)\)</span>, the
average cross-entropy loss is:</p>
<p><span class="math display">\[\mathfrak{L}(Z^{(2)}, y^\top) = -\frac{1}{m} \sum_{i=1}^m
\log(Z^{(2)}_{[i, y_i]})\]</span></p>
<p>which we want to minimize by adjusting the weights and biases.</p>
<h1 id="the-best-part-was-when-he-said-its-calculus-time-and-calculused-all-over-those-guys">the
best part was when he said “IT’S CALCULUS TIME” and calculused all over those guys</h1>
<p>We wish to compute loss gradients <span class="math inline">\(\frac{\partial
\mathcal{L}}{\partial W^{(k)}}\)</span>, <span class="math inline">\(\frac{\partial
\mathcal{L}}{\partial b^{(k)}}\)</span> for <span class="math inline">\(k = 1, 2\)</span>. For
simplicity, we begin by calculating partial derivatives for a single training example <span
class="math inline">\((x, y) \in \mathbb{R}^d \times [c]\)</span> (i.e. a minibatch of size 1). In
this case, the corresponding pre-activations and activations for the input <span
class="math inline">\(x\)</span>: (<span class="math inline">\(a^{(\ell)}, z^{(\ell)}\)</span> for
<span class="math inline">\(\ell = 1, 2\)</span>) are row vectors. Also, the single-example loss
<span class="math inline">\(\mathcal{L}\)</span> is defined by: <span
class="math inline">\(\mathcal{L} = - \log(z^{2}_y)\)</span>, so we have the immediate
derivatives:</p>
<p><span class="math display">\[\frac{\partial a^{(1)}_k}{\partial W^{(1)}_{ij}} =
\frac{\partial}{\partial W^{(1)}_{ij}} [x W^{(1)}_{[:,k]} + b^{(1)}_k] = \delta_{jk}
x_i\]</span></p>
<p><span class="math display">\[\frac{\partial a^{(1)}_k}{\partial b^{(1)}_j} =
\frac{\partial}{\partial b^{(1)}_j} [x W^{(1)}_{[:,k]} + b^{(1)}_k] = \delta_{jk}\]</span></p>
<p><span class="math display">\[\frac{\partial z^{(1)}_j}{\partial a^{(1)}_i} =
\frac{\partial}{\partial a^{(1)}_i} [\phi(a^{(1)}_j)] = \delta_{ij}
\phi&#39;(a^{(1)}_j)\]</span></p>
<p><span class="math display">\[\frac{\partial a^{(2)}_j}{\partial z^{(1)}_i } =
\frac{\partial}{\partial z^{(1)}_i} [z^{(1)} W^{(2)}_j + b^{(2)}_j] = W^{(2)}_{ij}\]</span></p>
<p><span class="math display">\[\frac{\partial a^{(2)}_k}{\partial W^{(2)}_{ij}} =
\frac{\partial}{\partial W^{(2)}_{ij}} [z^{(1)} W^{(2)}_{[:,k]} + b^{(2)}_k] = \delta_{jk}
z^{(1)}_i\]</span></p>
<p><span class="math display">\[\frac{\partial a^{(2)}_k}{\partial b^{(2)}_j} =
\frac{\partial}{\partial b^{(2)}_j} [z^{(1)} W^{(2)}_{[:,k]} + b^{(2)}_k] = \delta_{jk}\]</span></p>
<p><span class="math display">\[\frac{\partial z^{(2)}_j}{\partial a^{(2)}_i} =
\frac{\partial}{\partial a^{(2)}_i} [\text{softmax}(a^{(2)}_j)] = z^{(2)}_j (\delta_{ij} -
z^{(2)}_i)\]</span></p>
<p><span class="math display">\[\frac{\partial \mathcal{L}(z^{(2)}, y)}{\partial z^{(2)}_i} =
\frac{\partial}{\partial z^{(2)}_i} [-\log(z^{(2)}_y) ] = -
\frac{\delta_{iy}}{z^{(2)}_y}\]</span></p>
<p>Using the chain rule (a.k.a. backpropagation along the network), we can put these together and
obtain:</p>
<p><span class="math display">\[\frac{\partial \mathcal{L}}{\partial a^{(2)}_j} = \sum_{k \in [c]}
\frac{\partial \mathcal{L}}{\partial z^{(2)}_k} \frac{\partial z^{(2)}_k}{\partial a^{(2)}_j} = -
\sum_{k \in [c]} \frac{\delta_{ky}}{z^{(2)}_y} z^{(2)}_k (\delta_{jk} - z^{(2)}_j) = -(\delta_{yj} -
z^{(2)}_j) \]</span></p>
<p><span class="math display">\[\frac{\partial \mathcal{L}}{\partial W^{(2)}_{ij}} = \frac{\partial
\mathcal{L}}{\partial a^{(2)}_j} \frac{\partial a^{(2)}_j}{\partial W^{(2)}_{ij}} = -(\delta_{yj} -
z^{(2)}_j) z^{(1)}_i\]</span></p>
<p><span class="math display">\[\frac{\partial \mathcal{L}}{\partial b^{(2)}_j} = \frac{\partial
\mathcal{L}}{\partial a^{(2)}_j} \frac{\partial a^{(2)}_j}{\partial b^{(2)}_j} = -(\delta_{yj} -
z^{(2)}_j)\]</span></p>
<p><span class="math display">\[\frac{\partial \mathcal{L}}{\partial z^{(1)}_i} = \sum_{j \in [c]}
\frac{\partial \mathcal{L}}{\partial a^{(2)}_j} \frac{\partial a^{(2)}_j}{\partial z^{(1)}_i} = -
\sum_{j \in [c]} (\delta_{yj} - z^{(2)}_j) W_{ij}^{(2)} = -(\delta[y,:] - z^{(2)}) \cdot
W_{[i,:]}^{(2)}\]</span></p>
<p><span class="math display">\[\frac{\partial \mathcal{L}}{\partial a^{(1)}_j} = \frac{\partial
\mathcal{L}}{\partial z^{(1)}_j} \frac{\partial z^{(1)}_j}{\partial a^{(1)}_j} = \frac{\partial
\mathcal{L}}{\partial z^{(1)}_j} \cdot \phi&#39;(a^{(1)}_j)\]</span></p>
<p><span class="math display">\[\frac{\partial \mathcal{L}}{\partial W^{(1)}_{ij}} = \frac{\partial
\mathcal{L}}{\partial a^{(1)}_j} \frac{\partial a^{(1)}_j}{\partial W^{(1)}_{ij}} = \frac{\partial
\mathcal{L}}{\partial a^{(1)}_j} x_i\]</span></p>
<p><span class="math display">\[\frac{\partial \mathcal{L}}{\partial b^{(1)}_j} = \frac{\partial
\mathcal{L}}{\partial a^{(1)}_j} \frac{\partial a^{(1)}_j}{\partial b^{(1)}_j} = \frac{\partial
\mathcal{L}}{\partial a^{(1)}_j}\]</span></p>
<p>To vectorize, we can use:</p>
<p><span class="math display">\[\frac{\partial \mathcal{L}}{\partial b^{(2)}} = \frac{\partial
\mathcal{L}}{\partial a^{(2)}} = -(\delta[y, :] - z^{(2)})\]</span></p>
<p><span class="math display">\[\frac{\partial \mathcal{L}}{\partial W^{(2)}} = (z^{(1)})^\top
\frac{\partial \mathcal{L}}{\partial a^{(2)}} = - (z^{(1)})^\top (\delta[y, :] -
z^{(2)})\]</span></p>
<p><span class="math display">\[\frac{\partial \mathcal{L}}{\partial z^{(1)}} = - (\delta[y, :] -
z^{(2)}) W^{(2) \top} = \frac{\partial \mathcal{L}}{\partial a^{(2)}} W^{(2) \top} \]</span></p>
<p><span class="math display">\[\frac{\partial \mathcal{L}}{\partial b^{(1)}} = \frac{\partial
\mathcal{L}}{\partial a^{(1)}} = \frac{\partial \mathcal{L}}{\partial z^{(1)}} \odot
\phi&#39;(a^{(1)})\]</span></p>
<p><span class="math display">\[\frac{\partial \mathcal{L}}{\partial W^{(1)}} = x^\top \cdot
\frac{\partial \mathcal{L}}{\partial a^{(1)}}\]</span></p>
<p>The above is for a single training example. For a minibatch <span class="math inline">\((X,
y^\top) \in \mathbb{R}^{m \times d} \times [c]^{m \times 1}\)</span> of examples, we can average
over the examples, so that, if we define <span class="math inline">\(\mathcal{L}_k\)</span> to be
the single example loss for the <span class="math inline">\(k\)</span>-th training sample in the
batch, we get:</p>
<p><span class="math display">\[\mathfrak{L} = \frac{1}{m} \sum_{k=1}^m \mathcal{L}_k\]</span></p>
<p>Then:</p>
<p><span class="math display">\[\frac{\partial \mathfrak{L}}{\partial A^{(2)}_{[i,:]}} = \frac{1}{m}
\sum_{k=1}^m \frac{\partial \mathcal{L}_k}{\partial A^{(2)}_{[i, :]}} = \frac{1}{m} \frac{\partial
\mathcal{L}_i}{\partial A^{(2)}_{[i, :]}} = - \frac{1}{m} (Y_{[i,:]} - Z^{(2)}_{[i,:]})\]</span></p>
<p><span class="math display">\[\frac{\partial \mathfrak{L}}{\partial A^{(2)}} = - \frac{1}{m} (Y -
Z^{(2)})\]</span></p>
<p><span class="math display">\[\frac{\partial \mathfrak{L}}{\partial b^{(2)}} = \frac{1}{m}
\sum_{i=1}^m \frac{\partial \mathcal{L}_i}{\partial A^{(2)}_{[i,:]}} = - \frac{1}{m} \sum_{i=1}^m
(Y_{[i, :]} - Z^{(2)}_{[i, :]}) = \text{average along rows of } -(Y - Z^{(2)})\]</span></p>
<p><span class="math display">\[\frac{\partial \mathfrak{L}}{\partial W^{(2)}} = - \frac{1}{m}
\sum_{i=1}^m (Z^{(1)}_{[i,:]})^\top (Y_{[i, :]} - Z^{(2)}_{[i, :]}) = Z^{(1)\top} \frac{\partial
\mathfrak{L}}{\partial A^{(2)}}\]</span></p>
<p><span class="math display">\[\frac{\partial \mathfrak{L}}{\partial Z^{(1)}_{[i,:]}} = \frac{1}{m}
\sum_{k=1}^m \frac{\partial \mathcal{L}_k}{\partial Z^{(1)}_{[i,:]}} = - \frac{1}{m} (Y_{[i,:]} -
Z^{(2)}_{[i,:]}) W^{(2) \top} = \frac{\partial \mathfrak{L}}{\partial A^{(2)}_{[i,:]}} W^{(2) \top}
\]</span></p>
<p><span class="math display">\[\frac{\partial \mathfrak{L}}{\partial Z^{(1)}} = \frac{\partial
\mathfrak{L}}{\partial A^{(2)}} W^{(2) \top} \]</span></p>
<p><span class="math display">\[\frac{\partial \mathfrak{L}}{\partial A^{(1)}_{[i,:]}} = \frac{1}{m}
\sum_{k=1}^m \frac{\partial \mathcal{L}_k}{\partial A^{(1)}_{[i, :]}} = \frac{1}{m} \frac{\partial
\mathcal{L}_i}{\partial A^{(1)}_{[i, :]}} = \frac{1}{m} \frac{\partial \mathcal{L}_i}{\partial
Z^{(1)}_{[i, :]}} \odot \phi&#39;(A^{(1)}_{[i, :]}) = \frac{1}{m} \frac{\partial
\mathfrak{L}}{\partial Z^{(1)}_{[i, :]}} \odot \phi&#39;(A^{(1)}_{[i, :]})\]</span></p>
<p><span class="math display">\[\frac{\partial \mathfrak{L}}{\partial A^{(1)}} = \frac{1}{m}
\frac{\partial \mathfrak{L}}{\partial Z^{(1)}} \odot \phi&#39;(A^{(1)})\]</span></p>
<p><span class="math display">\[\frac{\partial \mathfrak{L}}{\partial b^{(1)}} = \frac{1}{m}
\sum_{i=1}^m \frac{\partial \mathcal{L}_i}{\partial b^{(1)}} = \frac{1}{m} \sum_{i=1}^m
\frac{\partial \mathcal{L}_i}{\partial Z^{(1)}_{[i,:]}} \odot \phi&#39;(A^{(1)}_{[i,:]}) =
\text{average along rows of } (\frac{\partial \mathfrak{L}}{\partial Z^{(2)}} \odot
\phi&#39;(A^{(1)}))\]</span></p>
<p><span class="math display">\[\frac{\partial \mathfrak{L}}{\partial W^{(1)}} = \frac{1}{m}
\sum_{k=1}^m \frac{\partial \mathcal{L}_k}{\partial W^{(1)}} = \frac{1}{m} \sum_{k=1}^m
(X_{[k,:]})^\top \frac{\partial \mathcal{L}_k}{\partial A^{(1)}_{[k,:]}} = \frac{1}{m} X^\top
\frac{\partial \mathfrak{L}}{\partial A^{(1)}}\]</span></p>
</body>
</html>
